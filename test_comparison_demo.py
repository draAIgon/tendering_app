"""
Demo de comparaciÃ³n de documentos
Prueba la funcionalidad de comparar mÃºltiples propuestas
"""

import sys
sys.path.append('./backend')

from utils.bidding import BiddingAnalysisSystem
import json
from pathlib import Path

def main():
    print("ğŸ”„ DEMO COMPARACIÃ“N DE DOCUMENTOS")
    print("=" * 50)
    
    # Inicializar sistema
    print("ğŸ“Š Inicializando sistema...")
    system = BiddingAnalysisSystem()
    system.initialize_system(provider="auto")
    print("âœ… Sistema listo")
    
    # Obtener documentos para comparar
    law_data_dir = Path("./LawData")
    documents = list(law_data_dir.glob("*.pdf")) + list(law_data_dir.glob("*.txt"))
    
    if len(documents) >= 2:
        # Tomar los primeros 2 documentos para comparar
        doc1 = str(documents[0])
        doc2 = str(documents[1])
        
        print(f"\nğŸ” Comparando documentos:")
        print(f"   ğŸ“„ Documento 1: {Path(doc1).name}")
        print(f"   ğŸ“„ Documento 2: {Path(doc2).name}")
        
        try:
            # Realizar comparaciÃ³n
            comparison_result = system.compare_proposals(
                proposal_paths=[doc1, doc2],
                comparison_criteria={
                    "structural_weight": 0.3,
                    "content_weight": 0.4,
                    "compliance_weight": 0.3
                }
            )
            
            print("âœ… ComparaciÃ³n completada")
            
            # Mostrar resultados
            print("\nğŸ“Š Resultados de la comparaciÃ³n:")
            
            # Resumen general
            if 'summary' in comparison_result:
                summary = comparison_result['summary']
                print(f"   ğŸ“ˆ Total documentos comparados: {summary.get('total_proposals', 0)}")
                print(f"   âœ… Comparaciones exitosas: {summary.get('successful_comparisons', 0)}")
            
            # AnÃ¡lisis individuales
            individual = comparison_result.get('individual_analyses', {})
            print(f"\nğŸ“‹ AnÃ¡lisis individuales:")
            for prop_id, analysis in individual.items():
                doc_id = analysis.get('document_id', 'N/A')
                print(f"   {prop_id}: {doc_id}")
                
                # Mostrar puntuaciones si estÃ¡n disponibles
                summary = analysis.get('summary', {})
                if summary:
                    compliance_score = summary.get('compliance_score', 0)
                    risk_score = summary.get('risk_score', 0)
                    print(f"      ğŸ“Š Cumplimiento: {compliance_score:.1f}%")
                    print(f"      âš ï¸ Riesgo: {risk_score:.1f}")
            
            # RecomendaciÃ³n
            if 'recommendation' in comparison_result:
                rec = comparison_result['recommendation']
                print(f"\nğŸ¯ RecomendaciÃ³n:")
                print(f"   ğŸ† Propuesta recomendada: {rec.get('recommended_proposal', 'N/A')}")
                print(f"   ğŸ“Š PuntuaciÃ³n: {rec.get('score', 0):.2f}")
                print(f"   ğŸ’¡ RazÃ³n: {rec.get('reason', 'N/A')}")
            
            # Errores si los hay
            errors = comparison_result.get('errors', [])
            if errors:
                print(f"\nâš ï¸ Advertencias:")
                for error in errors:
                    print(f"   â€¢ {error}")
            
        except Exception as e:
            print(f"âŒ Error en comparaciÃ³n: {e}")
            import traceback
            traceback.print_exc()
    
    else:
        print("âŒ Se necesitan al menos 2 documentos para comparar")
    
    print(f"\nğŸ‰ Demo de comparaciÃ³n completado!")

if __name__ == "__main__":
    main()
