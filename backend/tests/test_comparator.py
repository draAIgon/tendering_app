#!/usr/bin/env python3
"""
Test script for ComparatorAgent
Tests document comparison and similarity analysis capabilities
"""

import sys
import os
from pathlib import Path

# Agregar paths necesarios
current_dir = Path(__file__).parent
backend_dir = current_dir.parent  # Go up one level to backend directory
sys.path.append(str(backend_dir))
sys.path.append(str(backend_dir / "utils" / "agents"))

from utils.agents.comparator import ComparatorAgent
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_basic_comparison():
    """Test b√°sico de comparaci√≥n de documentos"""
    logger.info("=== Test B√°sico de Comparaci√≥n ===")
    
    # Buscar el documento de prueba
    backend_dir = current_dir.parent
    doc_paths = [
        backend_dir / "documents" / "EJEMPLO DE CONTRATO - RETO 1.pdf",
        backend_dir / ".." / "documents" / "EJEMPLO DE CONTRATO - RETO 1.pdf",
        backend_dir.parent / "EJEMPLO DE CONTRATO - RETO 1.pdf"
    ]
    
    document_path = None
    for path in doc_paths:
        if path.exists():
            logger.info(f"Documento encontrado en: {path}")
            document_path = str(path)
            break
    
    if not document_path:
        logger.warning("No se encontr√≥ documento para comparaci√≥n")
        return False
    
    try:
        # Crear agente comparador
        db_path = backend_dir / "db" / "test_comparator"
        agent = ComparatorAgent(vector_db_path=db_path)
        
        # Inicializar sistema de embeddings
        if agent.initialize_embeddings():
            logger.info("‚úÖ Sistema de embeddings inicializado")
        
        # Leer contenido del documento
        with open(document_path, 'rb') as f:
            # Para un test b√°sico, a√±adimos el mismo documento dos veces
            # En una implementaci√≥n real, tendr√≠amos documentos diferentes
            logger.info("Usando el mismo documento para auto-comparaci√≥n")
            
            # Simulamos tener contenido de texto (normalmente extra√≠do del PDF)
            content = "Contrato de obra p√∫blica entre PREFECTURA DEL GUAYAS y EDIFIKA S.A. Proyecto: Ampliaci√≥n de la V√≠a Samborond√≥n"
            
            # A√±adir documentos al sistema
            agent.add_document("doc1", content, "contract", {"source": "test1"})
            agent.add_document("doc2", content, "contract", {"source": "test2"})
            
            # Realizar comparaci√≥n de similitud de contenido
            comparison_result = agent.analyze_content_similarity("doc1", "doc2")
            
            logger.info("‚úÖ Comparaci√≥n de contenido exitosa")
            logger.info(f"üìä Similitud sem√°ntica: {comparison_result.get('semantic_similarity', 0):.2f}")
            
            # Verificar estructura del resultado
            required_keys = ['semantic_similarity', 'comparison_timestamp']
            for key in required_keys:
                if key in comparison_result:
                    logger.info(f"‚úÖ Clave encontrada: {key}")
                else:
                    logger.warning(f"‚ö†Ô∏è  Clave faltante: {key}")
            
            return True
        
    except Exception as e:
        logger.error(f"Error durante la comparaci√≥n: {e}")
        return False

def test_multiple_comparison():
    """Test de comparaci√≥n m√∫ltiple"""
    logger.info("\n=== Test de Comparaci√≥n M√∫ltiple ===")
    
    backend_dir = current_dir.parent
    
    try:
        # Crear agente comparador
        db_path = backend_dir / "db" / "test_comparator"
        agent = ComparatorAgent(vector_db_path=db_path)
        
        # Inicializar embeddings
        agent.initialize_embeddings()
        
        # Simular m√∫ltiples documentos
        documents = [
            ("doc1", "Contrato de obra p√∫blica para construcci√≥n de carretera", "contract"),
            ("doc2", "Propuesta t√©cnica para pavimentaci√≥n de v√≠as", "proposal"),
            ("doc3", "Especificaciones t√©cnicas para asfaltado", "specification")
        ]
        
        # A√±adir documentos
        for doc_id, content, doc_type in documents:
            agent.add_document(doc_id, content, doc_type)
        
        # Realizar comparaci√≥n m√∫ltiple
        doc_ids = [doc[0] for doc in documents]
        comparison_result = agent.compare_multiple_documents(doc_ids)
        
        logger.info("‚úÖ Comparaci√≥n m√∫ltiple exitosa")
        logger.info(f"üìä Documentos comparados: {len(doc_ids)}")
        
        # Verificar resultados
        if 'pairwise_comparisons' in comparison_result:
            comparisons = comparison_result['pairwise_comparisons']
            logger.info(f"üîç Comparaciones pareadas: {len(comparisons)}")
        
        return True
        
    except Exception as e:
        logger.error(f"Error en comparaci√≥n m√∫ltiple: {e}")
        return False

def test_similarity_analysis():
    """Test de an√°lisis de similitud"""
    logger.info("\n=== Test de An√°lisis de Similitud ===")
    
    backend_dir = current_dir.parent
    
    try:
        # Crear agente comparador
        db_path = backend_dir / "db" / "test_comparator"
        agent = ComparatorAgent(vector_db_path=db_path)
        
        # Inicializar embeddings
        if agent.initialize_embeddings():
            logger.info("‚úÖ Sistema de embeddings inicializado")
        
        # A√±adir documentos de prueba
        doc1_content = "Contrato para construcci√≥n de infraestructura vial con especificaciones t√©cnicas detalladas"
        doc2_content = "Propuesta de pavimentaci√≥n de carreteras con materiales de alta calidad"
        
        agent.add_document("test_doc1", doc1_content, "contract")
        agent.add_document("test_doc2", doc2_content, "proposal")
        
        # Realizar an√°lisis de similitud sem√°ntica
        similarity_result = agent.analyze_content_similarity("test_doc1", "test_doc2")
        
        logger.info("‚úÖ An√°lisis de similitud completado")
        
        # Verificar resultado
        if 'semantic_similarity' in similarity_result:
            similarity = similarity_result['semantic_similarity']
            logger.info(f"üìä Similitud sem√°ntica: {similarity:.3f}")
            
            if 0 <= similarity <= 1:
                logger.info("‚úÖ Valor de similitud en rango v√°lido")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è  Valor de similitud fuera de rango: {similarity}")
                return False
        else:
            logger.error("‚ùå No se encontr√≥ valor de similitud sem√°ntica")
            return False
        
    except Exception as e:
        logger.error(f"Error en an√°lisis de similitud: {e}")
        return False

def test_custom_criteria():
    """Test de criterios de comparaci√≥n personalizados"""
    logger.info("\n=== Test de Criterios de Comparaci√≥n ===")
    
    backend_dir = current_dir.parent
    
    try:
        # Crear agente comparador
        db_path = backend_dir / "db" / "test_comparator"
        agent = ComparatorAgent(vector_db_path=db_path)
        
        # Inicializar embeddings
        agent.initialize_embeddings()
        
        # A√±adir documentos de prueba
        doc1_content = "Especificaciones t√©cnicas: Utilizar concreto de 250 kg/cm¬≤. Plazo de ejecuci√≥n: 12 meses. Garant√≠a: 24 meses."
        doc2_content = "Propuesta t√©cnica: Concreto de 300 kg/cm¬≤. Tiempo de entrega: 10 meses. Garant√≠a extendida: 36 meses."
        
        agent.add_document("spec_doc", doc1_content, "specification")
        agent.add_document("prop_doc", doc2_content, "proposal")
        
        # Realizar an√°lisis de completitud t√©cnica
        technical_analysis = agent.analyze_technical_completeness("spec_doc", "prop_doc")
        
        logger.info("‚úÖ An√°lisis t√©cnico completado")
        
        # Verificar resultado
        if technical_analysis and 'technical_score' in technical_analysis:
            tech_score = technical_analysis['technical_score']
            logger.info(f"üìä Score t√©cnico: {tech_score:.2f}")
            
            # Realizar an√°lisis econ√≥mico
            economic_analysis = agent.analyze_economic_competitiveness("spec_doc", "prop_doc")
            
            if economic_analysis:
                logger.info("‚úÖ An√°lisis econ√≥mico completado")
                logger.info("‚úÖ Criterios personalizados funcionando correctamente")
                return True
            else:
                logger.warning("‚ö†Ô∏è  An√°lisis econ√≥mico incompleto")
                return False
        else:
            logger.error("‚ùå An√°lisis t√©cnico fall√≥")
            return False
        
    except Exception as e:
        logger.error(f"Error en criterios personalizados: {e}")
        return False

def main():
    """Funci√≥n principal del test"""
    logger.info("üöÄ Iniciando tests del ComparatorAgent")
    
    tests = [
        ("Comparaci√≥n B√°sica", test_basic_comparison),
        ("Comparaci√≥n M√∫ltiple", test_multiple_comparison),
        ("An√°lisis de Similitud", test_similarity_analysis),
        ("Criterios Personalizados", test_custom_criteria)
    ]
    
    results = []
    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"üß™ Ejecutando: {test_name}")
        logger.info('='*50)
        
        try:
            success = test_func()
            results.append((test_name, success))
            
            if success:
                logger.info(f"‚úÖ {test_name} completado exitosamente")
            else:
                logger.error(f"‚ùå {test_name} fall√≥")
                
        except Exception as e:
            logger.error(f"üí• Error cr√≠tico en {test_name}: {e}")
            results.append((test_name, False))
    
    # Resumen final
    logger.info(f"\n{'='*50}")
    logger.info("üìä RESUMEN DE TESTS")
    logger.info('='*50)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        logger.info(f"  {status} {test_name}")
    
    logger.info(f"\nüèÜ Resultado final: {passed}/{total} tests exitosos")
    
    if passed == total:
        logger.info("üéâ ¬°Todos los tests pasaron!")
    else:
        logger.warning(f"‚ö†Ô∏è  {total - passed} tests fallaron")

if __name__ == "__main__":
    main()
